Elemental
=========

Elemental is a high-level high-performance framework for distributed-memory 
dense linear algebra that is inspired by [PLAPACK][1] and, more recently, 
[FLAME][2].

The key idea behind the project is that an appropriate level of abstraction 
allows one to focus on *algorithms* rather than getting tied down in 
implementation details.

License
=======
New BSD License

Current Status
==============
- The Level 3 BLAS is fully functional, as well as Cholesky, LU, Householder 
  tridiagonalization, and  triangular inversion. Gaussian elimination has been 
  implemented but has not yet been tested. Performance is generally superior 
  to PLAPACK and ScaLAPACK.

Build
=====
The build system is not yet automated. However, all that is currently required
is to define the appropriate MPI compiler and BLAS/LAPACK libraries at the 
top of the Makefile.

There are two main build modes, *debug* and *release*. The former maintains a 
call stack and performs judicious error-checking. Thus debug-mode should be used
when testing a new algorithm. You can build it with
      make debug
If you would also like to build the debug test drivers for the distributed 
BLAS and LAPACK routines, run
      make test-debug

Similarly, you can build the baremetal version of the library by running 
      make release
The command
      make test-release
perform the equivalent action as in the debug case.

If neither debug nor release modes are specified, both are built. The relevant
commands are
      make
      make test

Overview
--------
The project is designed with the knowledge that distribution blocksizes do not 
have to equal algorithmic blocksizes. With this in mind, Elemental distributes 
matrices element by element for performance robustness: unaligned distributions
are *cheaply* and *automatically* fixed via a bijection implemented through 
MPI's SendRecv.

Taking cues from [PLAPACK][1], Elemental does not focus solely on manipulating 
traditional 2d matrix distributions, but is designed to use 1d (or *vector*) 
distributions when appropriate. For instance, when solving a triangular 
system with many right-hand sides, it is extremely efficient to solve with 
each right-hand side owned by a single process.

Since moving between different distributions is a necessary part of 
distributed memory algorithms, we introduce the following notation:

*  If a matrix `A` has each column distributed in a manner denoted by the symbol
   `X`, and each row distributed in a manner denoted by `Y`, then we write 
   `A[X,Y]` to represent the distribution.
*  A typical 2d matrix distribution is written `A[MC,MR]`, where `MC` means 
   "distributed like a *Matrix* *Column*", and `MR` means "distributed like a 
   *Matrix* *Row*."
*  Similarly, `A[MR,MC]` will denote the transposed matrix distribution.
*  `VC` represents a *Vector* distribution generated by a 1d *Column*-major 
   wrapping of the 2d process grid.
*  `VR` represents a *Vector* distribution generated by a 1d *Row*-major 
   wrapping of the 2d process grid.
*  `*` represents no distribution (complete duplication on all processes)
*  Thus `A[*,VC]` represents a matrix `A` where each column is owned by a 
   single process and they have been ordered over the process grid in a 
   column-major fashion.

Basic Example
-------------
Elemental has a templated distributed matrix class, `DistMatrix`, that allows 
for the specification of these distributions. We can create an `m x n` matrix 
in a matrix distribution over MPI_COMM_WORLD with:

    Grid g( MPI_COMM_WORLD );
    DistMatrix<double,MC,MR> A(m,n,g);

Similarly, we can create an `m x n` matrix in a transposed matrix distribution
using

    DistMatrix<double,MR,MC> A(m,n,g);

Redistribution is handled automatically via overloading the `=` operator:

    DistMatrix<double,MR,MC  > A(m,n,g);
    DistMatrix<double,VC,Star> B(m,n,g);
    // Fill matrix A...
    B = A;

[1]: http://www.cs.utexas.edu/users/plapack/new/using.html
[2]: http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage

